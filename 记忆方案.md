# ATRI记忆系统开发方案

## 一、项目背景

为了实现具有长期记忆能力的AI助手，需要设计一套完整的记忆管理系统。该系统能够记录、存储、检索和更新与用户的互动信息，让AI能够建立连贯的对话体验，记住用户的偏好、历史事件和关系进展。

## 二、核心理念

ATRI的记忆系统基于**分层存储+智能检索**的架构，模拟人类记忆机制：

- **短期记忆（工作记忆）**：当前对话上下文，类似人的"瞬时记忆"
- **中期记忆（日记系统）**：每日重要事件摘要，类似"情景记忆"
- **长期记忆（用户档案）**：稳定的用户特征和偏好，类似"语义记忆"

## 三、系统架构

### 3.1 整体架构图

```
┌─────────────────────────────────────┐
│         应用层 (Chat API)           │  - 用户交互接口
├─────────────────────────────────────┤
│  记忆管理层 (Agent/Context Service) │  - 记忆检索和注入
├─────────────────────────────────────┤
│  存储抽象层 (Data Service)          │  - 数据访问接口
├─────────────────────────────────────┤
│  向量存储 + 结构化存储              │  - 数据持久化
└─────────────────────────────────────┘
```

### 3.2 数据流设计

```
用户输入 → 实时记录 → 工作记忆 → AI处理
    ↓           ↓         ↓        ↓
对话日志 → 日记生成 → 向量化 → 档案更新
    ↓           ↓         ↓        ↓
   短期   ←   中期   ←   长期记忆
```

### 3.3 记忆数据模型

```typescript
interface MemoryRecord {
  id: string;           // 唯一标识
  userId: string;       // 用户隔离
  category: MemoryType; // 记忆类型
  content: string;      // 原始内容
  embedding?: number[]; // 向量表示
  metadata: {           // 元数据
    importance: number; // 重要性 1-10
    timestamp: number;  // 时间戳
    tags?: string[];    // 标签
    expires?: number;   // 过期时间
    emotion?: string;   // 情感标记
  };
}

type MemoryType =
  | 'conversation'  // 对话记录
  | 'diary'        // 日记
  | 'profile'      // 用户档案
  | 'fact'         // 事实
  | 'preference';  // 偏好
```

## 四、技术方案

### 4.1 存储技术选型

| 存储类型 | 适用场景 | 推荐方案 | 成本 |
|---------|---------|---------|------|
| 向量数据库 | 语义搜索 | Pinecone/Weaviate | 中高 |
| 文档数据库 | 对话记录 | MongoDB/PostgreSQL | 中 |
| 缓存 | 工作记忆 | Redis | 低 |
| 对象存储 | 文件备份 | S3/OSS | 低 |

根据规模选择策略：
- 小规模（<100万向量）：Chroma/FAISS（本地部署）
- 中规模（100-1000万）：Pinecone/Weaviate（托管服务）
- 大规模（>1000万）：Milvus/Qdrant（自建集群）

### 4.2 Embedding模型选择

| 模型 | 维度 | 成本 | 质量 | 适用场景 |
|------|------|------|------|---------|
| OpenAI text-embedding-3-small | 1536 | $0.02/1M | 高 | 生产环境 |
| BGE-M3 | 1024 | 免费 | 中高 | 中文优化 |
| Sentence-Transformers | 768 | 免费 | 中 | 快速原型 |

### 4.3 记忆检索算法

```
记忆检索权重分配：
1. 语义相似度（向量匹配） - 权重70%
2. 时间新鲜度（最近优先） - 权重20%
3. 重要性评分（高价值优先） - 权重10%

检索流程：
输入：用户查询 + 用户ID
1. 向量化查询文本
2. 在向量数据库中搜索Top-K
3. 应用时间衰减函数
4. 过滤低重要性记忆
5. 多样性保证（避免同类型记忆过多）
输出：相关记忆列表
```

## 五、功能设计

### 5.1 核心功能模块

#### 5.1.1 记忆存储模块
- 实时对话记录
- 批量记忆写入
- 增量更新机制
- 去重和合并

#### 5.1.2 记忆检索模块
- 语义搜索
- 关键词搜索
- 时间范围筛选
- 分类过滤

#### 5.1.3 记忆管理模块
- 重要性评分
- 自动过期清理
- 用户编辑接口
- 导入导出功能

#### 5.1.4 上下文构建模块
- 记忆摘要生成
- 上下文长度控制
- 优先级排序
- 情感标记传递

### 5.2 AI集成方案

#### 5.2.1 记忆注入方式

**方案1：直接注入到Prompt**
```python
def build_context_prompt(user_message: str, memories: List[Memory]):
    prompt = f"""你是AI助手，以下是相关的历史记忆：

{format_memories(memories)}

当前对话：{user_message}

请基于这些记忆回答。"""
    return prompt
```

**方案2：使用RAG（检索增强生成）**
- 检索相关记忆片段
- 构建知识库上下文
- 生成基于事实的回答

**方案3：使用LangChain记忆组件**
- ConversationBufferMemory：短期对话
- VectorStoreRetrieverMemory：长期记忆
- CombinedMemory：混合记忆策略

#### 5.2.2 智能记忆更新

```python
class MemoryUpdater:
    async def update_after_conversation(self, conversation: List[Dict]):
        # 1. LLM提取关键信息
        extraction_prompt = "从对话中提取需要记住的信息..."
        extracted = await self.llm.generate(extraction_prompt)

        # 2. 分类和评分
        for item in extracted:
            category = await self.classify_memory(item)
            importance = await self.score_importance(item)

            # 3. 存储到向量数据库
            await self.store_with_priority(item, category, importance)
```

## 六、开发计划

### Phase 1：基础功能（2周）
- [ ] 基础对话记录存储（SQLite/PostgreSQL）
- [ ] 简单的关键词搜索
- [ ] 基础的上下文注入
- [ ] API接口设计

### Phase 2：向量化搜索（3周）
- [ ] 集成向量数据库（Pinecone/Chroma）
- [ ] 实现语义搜索
- [ ] 记忆重要性评分系统
- [ ] 批量处理优化

### Phase 3：智能记忆管理（4周）
- [ ] 自动记忆提取和分类
- [ ] 记忆衰减和更新机制
- [ ] 个性化记忆展示界面
- [ ] 隐私控制和数据脱敏

### Phase 4：优化和扩展（2周）
- [ ] 性能监控和优化
- [ ] 成本控制机制
- [ ] 多语言支持
- [ ] 高级检索功能

## 七、技术实现示例

### 7.1 基础框架

```python
class MemorySystem:
    def __init__(self, config):
        self.vector_db = init_vector_db(config)
        self.embed_model = init_embedding_model(config)
        self.llm_client = init_llm_client(config)

    async def store_memory(self, content: str, metadata: dict):
        # 生成向量
        embedding = await self.embed_model.embed(content)

        # 存储记录
        memory = {
            "id": generate_id(),
            "content": content,
            "embedding": embedding,
            "metadata": metadata
        }
        await self.vector_db.upsert([memory])

    async def search_memories(self, query: str, filters: dict = None):
        # 向量搜索
        query_embedding = await self.embed_model.embed(query)
        results = await self.vector_db.query(
            vector=query_embedding,
            filter=filters,
            top_k=10
        )
        return self.rank_results(results)
```

### 7.2 上下文构建

```python
def build_context(memories: List[Memory], limit: int = 2000):
    """构建上下文，控制总长度"""
    context_parts = []
    current_length = 0

    # 按重要性排序
    sorted_memories = sorted(memories, key=lambda m: m.importance, reverse=True)

    for memory in sorted_memories:
        memory_text = f"[{memory.category}] {memory.content}"
        if current_length + len(memory_text) > limit:
            break
        context_parts.append(memory_text)
        current_length += len(memory_text)

    return "\n".join(context_parts)
```

### 7.3 性能优化

```python
# 批量处理
class BatchProcessor:
    def __init__(self, batch_size=100):
        self.batch_size = batch_size
        self.pending = []

    async def add(self, item):
        self.pending.append(item)
        if len(self.pending) >= self.batch_size:
            await self.flush()

    async def flush(self):
        if self.pending:
            await self.process_batch(self.pending)
            self.pending.clear()

# 缓存机制
class MemoryCache:
    def __init__(self, ttl=3600):
        self.cache = {}
        self.ttl = ttl

    def get(self, key):
        item = self.cache.get(key)
        if item and time.time() - item['timestamp'] < self.ttl:
            return item['data']
        return None

    def set(self, key, data):
        self.cache[key] = {
            'data': data,
            'timestamp': time.time()
        }
```

## 八、安全和隐私

### 8.1 数据安全

- **加密存储**：敏感数据AES加密
- **传输安全**：HTTPS/TLS加密
- **访问控制**：基于用户ID的隔离
- **审计日志**：记录所有访问操作

### 8.2 隐私保护

- **自动脱敏**：识别和屏蔽敏感信息
- **用户控制**：提供删除和编辑接口
- **数据最小化**：只存储必要信息
- **合规性**：符合GDPR等法规

### 8.3 脱敏规则

```python
SENSITIVE_PATTERNS = {
    'phone': r'\b1[3-9]\d{9}\b',
    'email': r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b',
    'id_card': r'\b\d{17}[\dX]\b',
    'credit_card': r'\b\d{4}[-\s]?\d{4}[-\s]?\d{4}[-\s]?\d{4}\b'
}

def sanitize_content(content: str) -> str:
    for pattern_name, pattern in SENSITIVE_PATTERNS.items():
        content = re.sub(pattern, f'[{pattern_name.upper()}]', content)
    return content
```

## 九、监控和维护

### 9.1 关键指标

- **性能指标**
  - 检索响应时间
  - 存储写入延迟
  - 向量计算耗时

- **质量指标**
  - 检索准确率
  - 用户满意度
  - 记忆覆盖率

- **成本指标**
  - API调用成本
  - 存储费用
  - 计算资源消耗

### 9.2 告警策略

```python
class MonitoringSystem:
    def __init__(self):
        self.thresholds = {
            'response_time': 1000,  # ms
            'error_rate': 0.01,     # 1%
            'daily_cost': 100       # USD
        }

    async def check_metrics(self):
        metrics = await self.collect_metrics()

        if metrics['avg_response_time'] > self.thresholds['response_time']:
            await self.send_alert('Response time too high!')

        if metrics['error_rate'] > self.thresholds['error_rate']:
            await self.send_alert('Error rate exceeded!')
```

## 十、创新方向

### 10.1 记忆遗忘机制
- 模拟人类艾宾浩斯遗忘曲线
- 重要记忆自动强化
- 低价值记忆自然衰减

### 10.2 情感记忆
- 记录对话中的情绪变化
- 情感上下文影响回复风格
- 情感记忆的长期追踪

### 10.3 跨会话连续性
- 会话恢复功能
- 上下文无缝衔接
- 长期关系建立

### 10.4 记忆可视化
- 时间线展示互动历史
- 记忆热力图
- 关系进展可视化

### 10.5 记忆共享
- 用户选择性分享记忆
- 群组记忆功能
- 记忆导入导出

## 十一、风险评估

### 11.1 技术风险
- 向量数据库性能瓶颈
- Embedding模型漂移
- 大规模数据管理复杂度

### 11.2 成本风险
- API调用费用超预期
- 存储成本线性增长
- 计算资源需求上升

### 11.3 风险缓解措施
- 实施成本监控和预警
- 多级缓存减少API调用
- 定期清理低价值数据

## 十二、总结

ATRI记忆系统通过分层存储、智能检索和持续更新，实现了AI的长期记忆能力。该系统不仅提高了对话的连贯性和个性化，也为建立更深层次的人机关系奠定了基础。

关键成功因素：
1. 合理的架构设计
2. 高效的检索算法
3. 严格的安全控制
4. 持续的优化迭代
5. 良好的用户体验

通过这套系统，AI助手将能够真正"记住"用户，提供更贴心、更有温度的服务。